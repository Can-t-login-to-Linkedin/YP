{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd090ff-f204-4668-b4e4-0149eaac2a68",
   "metadata": {},
   "source": [
    "# Парсеры для скрапинга информации с сайта LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57afe70f-9898-48bb-922f-7092d513821c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вот эта штука не оставляет следов по которым всякие антиботы\n",
    "# могут нас поймать\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.parse import unquote\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba0355c-df96-4d3d-abbd-33f9991086e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_linkedin(filename='path_to_the_file.txt'):\n",
    "    \"\"\"\n",
    "    Opens undetected chromedriver\n",
    "    and log in to linkedIn\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename :\n",
    "         (Default value = 'path_to_the_file.txt')\n",
    "         A path to the txt file where\n",
    "         login and password are stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        undetected webdriver\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as handler:\n",
    "        lines = handler.readlines()\n",
    "        MAIL, PASS = lines[0].strip(), lines[1]\n",
    "    \n",
    "    \n",
    "    driver = uc.Chrome()\n",
    "    driver.get(\"https://linkedin.com/uas/login\")\n",
    "    time.sleep(3)\n",
    "    login_field = driver.find_element(By.ID,\"username\")\n",
    "    login_field.send_keys(MAIL)\n",
    "\n",
    "    pass_field = driver.find_element(By.ID,\"password\")\n",
    "    pass_field.send_keys(PASS)\n",
    "    pass_field.submit()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6fef1db-5aab-4d70-8e91-5fc4346dddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def profiles_search_results(driver, \n",
    "                            query:str, \n",
    "                            pages_num:int = 5)->list:\n",
    "    \"\"\"\n",
    "    Search profilles according to the\n",
    "    search query\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        undetected webdriver\n",
    "    query : str\n",
    "        Query search\n",
    "    pages_num : int\n",
    "         (Default value = 5)\n",
    "        Number of pages to scrape\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #сюда складываю ссылки на профили\n",
    "    profiles_links = []\n",
    "    for page in range(1, pages_num+1):\n",
    "        # нажатие кнопки next  у меня не работает, поэтому просто меняю страницу в самой url\n",
    "        # локация Россия\n",
    "        driver.get(f'https://www.linkedin.com/search/results/people/?geoUrn=%5B%22101728296%22%5D&keywords={query}' +\n",
    "                   f'&origin=FACETED_SEARCH&page={page}&sid=wMe')\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        # тут просто собираю ссылки\n",
    "        for person in soup.find_all('span', attrs={'class': 'entity-result__title-text'}):\n",
    "            link = person.find('a', attrs={'class': 'app-aware-link'})['href']\n",
    "            profiles_links.append(link)\n",
    "        # прокрутка до конца страницы для имитации реального пользователя\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(1,5))\n",
    "\n",
    "    return profiles_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29349d1-9da0-4d6d-b18f-7700ceeeacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_emploeeys(driver, \n",
    "                          companies:list, \n",
    "                          n_scrolls:int = 1)->list:\n",
    "    \"\"\"\n",
    "    Search profiles according to the\n",
    "    companies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        undetected webdriver\n",
    "    companies : list\n",
    "        list of companies to search for\n",
    "    n_scrolls : int\n",
    "         (Default value = 1)\n",
    "        Number of pages to scrape\n",
    "    Returns\n",
    "    -------\n",
    "    companies_employeers: dict\n",
    "        dict of companies and their\n",
    "        employee's links\n",
    "    \"\"\"\n",
    "    companies_employeers = {}\n",
    "    for company in companies: \n",
    "        employees_links = []\n",
    "        driver.get(f'https://www.linkedin.com/company/{company}/people/') \n",
    "        \n",
    "        time.sleep(15) \n",
    "        # scroll + timeout \n",
    "        for _ in range(n_scrolls):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(5,10)) \n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        # org-people-profile-card__profile-info\n",
    "        \n",
    "        #сбор ссылок\n",
    "        for index, person in enumerate(soup.find_all('div', attrs={'class': 'org-people-profile-card__profile-info'})):\n",
    "            #print(index)\n",
    "            try:\n",
    "                image = person.find('div', attrs={'class':'artdeco-entity-lockup__image'})\n",
    "                link = image.find('a', attrs={'class': 'app-aware-link'})['href']\n",
    "                employees_links.append(link)\n",
    "            except:\n",
    "                continue\n",
    "        companies_employeers[company] = employees_links\n",
    "        time.sleep(random.uniform(1,5))  \n",
    "          \n",
    "    return companies_employeers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b77b89-3fdc-47c3-9c4c-028f0f6fa2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_company_and_experience(driver)->tuple:\n",
    "    \"\"\"\n",
    "    Scrapes the experience page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        undetected webdriver\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of strings:\n",
    "        company name\n",
    "        years of total experience\n",
    "        employment type\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #переходим на страницу опыта\n",
    "    experience_page = driver.current_url + 'details/experience/'\n",
    "    driver.get(experience_page)\n",
    "    time.sleep(random.uniform(1,5))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    total = 0\n",
    "    #вот тут будут строчки из каждой позиции в опыте\n",
    "    for i in soup.find_all('span', {'class':'t-14 t-normal t-black--light'}):\n",
    "        # собираю тексты\n",
    "        t = i.find('span', {'class': 'visually-hidden'}).text\n",
    "        #не всегда есть год, месяц, иногда вообще пусто\n",
    "        try:\n",
    "            total += int(re.search(r'\\d+ mo', t)[0].split()[0])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            total += int(re.search(r'\\d+ yr', t)[0].split()[0]) * 12\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        curr_job = (soup.find_all('li', {'class':'pvs-list__paged-list-item'})[0]\n",
    "                         .find('span', 't-14 t-normal')\n",
    "                         .find('span', 'visually-hidden')).text.strip()\n",
    "        curr_company = curr_job.split(' · ')[0]\n",
    "    except: \n",
    "        curr_job = None\n",
    "        curr_company = None\n",
    "    try:\n",
    "        employment_type = curr_job.split(' · ')[1]\n",
    "    except:\n",
    "        employment_type = 'unknown'\n",
    "        \n",
    "    years = round(total / 12, 2)\n",
    "    driver.back()\n",
    "    \n",
    "    return curr_company, years, employment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f76d48-a85a-49b9-af59-8c5a22fce4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_skills(driver)->list:\n",
    "    \"\"\"\n",
    "    Scrapes the skills page\n",
    "    to get the skills list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        undetected webdriver\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    skills:\n",
    "        list of profiles's skills \n",
    "    \"\"\"\n",
    "    \n",
    "    curr_link = driver.current_url\n",
    "    \n",
    "    driver.get(curr_link + 'details/skills/')\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    \n",
    "    skills = []\n",
    "    raw_skills = soup.find('main', 'scaffold-layout__main').find_all('div', 'pvs-entity')\n",
    "    \n",
    "    for skill in raw_skills:\n",
    "        skills.append(skill.find('span', 'visually-hidden').text)\n",
    "        \n",
    "    driver.back()\n",
    "    \n",
    "    return list(set(skills))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad64d515-ea9d-4d6b-a742-59ddfd7ffa18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_profile_info(driver, \n",
    "                     link:str)->tuple:\n",
    "    \"\"\"\n",
    "    Scrapes the main profile page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        undetected chromedriver\n",
    "        \n",
    "    link : str\n",
    "        Link for the profile's page\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of str and dict:\n",
    "        u_id - unique person's id\n",
    "        person - dictionary with all \n",
    "        scraped info\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    driver.get(link)\n",
    "    time.sleep(random.uniform(2,5))\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    person = {}\n",
    "    u_id = unquote(link.split('in/')[1].split('?')[0])\n",
    "    person['name'] = soup.find('h1', {'class': 'text-heading-xlarge'}).text\n",
    "    person['city'] = soup.find('span',\"text-body-small inline t-black--light break-words\").text.strip().split(',')[0]\n",
    "    try:\n",
    "        person['connections'] = re.search('\\d+. connections|\\d+ connections', \n",
    "                                          soup.find('div', 'ph5')\n",
    "                                .text)[0].split()[0]\n",
    "    except:\n",
    "        person['connections'] = '0'\n",
    "    role = soup.find('div', {'class': 'text-body-medium break-words'}).text.strip()\n",
    "    person['role'] = (role.split(' at ')[0]\n",
    "                      .split(' - ')[0]\n",
    "                      .split(' – ')[0]\n",
    "                      .split(' as ')[0])\n",
    "    \n",
    "    person_info = get_company_and_experience(driver)\n",
    "    person['company'] = person_info[0]\n",
    "    person['experience'] = person_info[1]\n",
    "    person['employment_type'] = person_info[2]\n",
    "    \n",
    "    person['skills'] = get_skills(driver)\n",
    "\n",
    "    return u_id, person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e08930-3bcd-4557-b7cc-1689e5b5941d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape_posts_reactions(driver, \n",
    "                           link:str, \n",
    "                           num_scrolls=5)->list:\n",
    "    \"\"\"\n",
    "    Scrapes the posts or reactions\n",
    "    page.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        undetected chromedriver\n",
    "    link : str\n",
    "        profile's link\n",
    "    num_scrolls :\n",
    "         (Default value = 5)\n",
    "        number of max scrolls to scroll\n",
    "        on the posts/reactions pages\n",
    "    Returns\n",
    "    -------\n",
    "    posts_texts: list\n",
    "        A list of texts\n",
    "    \"\"\"\n",
    "    \n",
    "    driver.get(link)\n",
    "    time.sleep(random.uniform(3,5))\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    #прокрутка до конца страницы\n",
    "    for i in range(num_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(1,5))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    # собираю тексты\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    posts = soup.find_all('li', class_='profile-creator-shared-feed-update__container')\n",
    "    posts_texts = []\n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_text = (post.find('div', {'class': 'feed-shared-update-v2__description-wrapper mr2'})\n",
    "                         .find('span', {'dir': 'ltr'}).text)\n",
    "            posts_texts.append(post_text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return posts_texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a847489-3836-4d15-80c2-1ff8b13f0b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_posts_reactions(driver, \n",
    "                        link:str)->tuple:\n",
    "    \"\"\"\n",
    "    Goes to posts and reactions page and\n",
    "    scrapes them successively\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        undetected chromedriver\n",
    "    link : str\n",
    "        profile's link\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of lists:\n",
    "        posts and reactions\n",
    "    \"\"\"\n",
    "    \n",
    "    #тут есть два раздела: posts и reactions\n",
    "    time.sleep(random.uniform(1,5))\n",
    "    \n",
    "    posts_url = driver.current_url + 'recent-activity/all/'\n",
    "    reactions_url = driver.current_url + 'recent-activity/reactions/'\n",
    "    # тексты для каждого\n",
    "    posts = scrape_posts_reactions(driver, posts_url)\n",
    "    time.sleep(random.uniform(1,5))\n",
    "    react_posts = scrape_posts_reactions(driver, reactions_url)\n",
    "        \n",
    "    return posts, react_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9ce7a-d38b-43b2-9aec-81c61459cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_profiles(search_query:str, \n",
    "                   filename:str,\n",
    "                   num_of_profiles=85)-> tuple:\n",
    "    \"\"\"\n",
    "    Main function to parse the profiles\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_query : str\n",
    "        input search query\n",
    "    filename : str\n",
    "        file where login and password\n",
    "        are stored\n",
    "    num_of_profiles :\n",
    "         (Default value = 85)\n",
    "         number of profiles to scrape\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # соединяю запрос в одну строку заменяя пробелы\n",
    "    search_query = '%20'.join(search_query.split())\n",
    "    #открываю линкдин\n",
    "    driver = open_linkedin(filename)\n",
    "    #собираю ссылки со страницы резцльтатов поиска\n",
    "    links = profiles_search_results(driver, search_query, pages_num=30)\n",
    "    links = [link for link in links if 'in/' in link]\n",
    "    # словарь профилей с информацией о них\n",
    "    profiles = {}\n",
    "    # словари с постами пользователей\n",
    "    # и с постами на которые реагировал пользователь\n",
    "    u_posts, u_reactions = {}, {}\n",
    "\n",
    "    for count, link in tqdm(enumerate(links[:])):\n",
    "        u_id, profile = get_profile_info(driver, link)\n",
    "        profiles[u_id] = profile\n",
    "        posts, reactions = get_posts_reactions(driver, link)\n",
    "        u_posts[u_id] = posts\n",
    "        u_reactions[u_id] = reactions\n",
    "        if count == num_of_profiles:\n",
    "            break\n",
    "    \n",
    "    return profiles, u_posts, u_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cda1719-62b0-4c8f-98bf-1b7f9c42ff63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scroll_reactions_window(driver,\n",
    "                            max_scrolls:int = 40):\n",
    "    \"\"\"\n",
    "    Scroll the reactions window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    driver :\n",
    "        \n",
    "    max_scrolls : int\n",
    "         (Default value = 40)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    js_code = \"return document.getElementsByClassName('artdeco-modal__content')[0].scrollHeight\"\n",
    "    last_height = driver.execute_script(js_code)\n",
    "    count = 0\n",
    "    while count < max_scrolls:\n",
    "        time.sleep(random.uniform(1,3))\n",
    "        scroll_script = 'arguments[0].scrollTop = arguments[0].scrollHeight'\n",
    "        driver.execute_script(scroll_script, \n",
    "                              driver.find_element(By.CLASS_NAME,'artdeco-modal__content'))\n",
    "        time.sleep(random.uniform(2,4))\n",
    "        new_height = driver.execute_script(js_code)\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8474d-19a3-4410-9e8e-4b4a7b0b6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_posts_by_query(search_query:str,\n",
    "                         filename:str,\n",
    "                         max_posts:int = 150)->dict:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_query : str\n",
    "        search query text\n",
    "    filename : str\n",
    "        filename with the login info\n",
    "    max_posts : int\n",
    "         (Default value = 150)->dictdriver = open_linkedin(filename)\n",
    "        max posts to scrape\n",
    "    Returns\n",
    "    -------\n",
    "    result_dict: dict\n",
    "        A dictionary with all results\n",
    "    \"\"\"\n",
    "\n",
    "    driver = open_linkedin(filename)\n",
    "    \n",
    "    # соединяю его в одну строку заменяя пробелы\n",
    "    search_query = '%20'.join(search_query.split())\n",
    "\n",
    "    #пустой словарь куда будем скидывать результаты\n",
    "    result_dict = {}\n",
    "\n",
    "    # переходим на страницу с нужным поисковым запросом\n",
    "    driver.get(f'https://www.linkedin.com/search/results/content/?keywords={search_query}&origin=SWITCH_SEARCH_VERTICAL')\n",
    "    time.sleep(2)\n",
    "\n",
    "    #создаем 2 списка, в каждом из них элементы при нажатии на которые открываются лайки\n",
    "    likes_list_1 = driver.find_elements(By.CLASS_NAME, 'social-details-social-counts__reactions-count')\n",
    "    likes_list_2 = driver.find_elements(By.CLASS_NAME, 'social-details-social-counts__social-proof-text')\n",
    "\n",
    "    #каунтер обозначает номер поста, 2 следующих указателя определяют номера элементов реакций\n",
    "    count = 0\n",
    "    reactions_button_counter_1 = 0\n",
    "    reactions_button_counter_2 = 0\n",
    "    # переменная soup отвечает за содержимое всей страницы\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    \n",
    "    while len(result_dict)<=max_posts:\n",
    "        time.sleep(random.uniform(2,4))\n",
    "        # переменная post_card отвечает за содержимое карточки поста\n",
    "        post_card = soup.find_all('div','pt1 mb2 artdeco-card')[count]\n",
    "        try:\n",
    "            #пытаемся вытащить текст поста\n",
    "            text_relative_feed = post_card.find('div','update-components-text relative feed-shared-update-v2__commentary')\n",
    "            post_text = text_relative_feed.find('span',{'dir':'ltr'}).text\n",
    "            result_dict[count] = [post_text,{}]\n",
    "        except:\n",
    "            #если текста нет и в посте только картинка, то так и пишем\n",
    "            result_dict[count] = ['Just image in post',{}]\n",
    "            print('Just image in post')\n",
    "\n",
    "        #проверяем условие по которому делается клик на соответствующую кнопку с лайками\n",
    "        if 'social-details-social-counts__reactions-count' in str(post_card):\n",
    "            likes_list_1[reactions_button_counter_1].click()\n",
    "            reactions_button_counter_1 += 1\n",
    "        elif 'social-details-social-counts__social-proof-text' in str(post_card):\n",
    "            likes_list_2[reactions_button_counter_2].click()\n",
    "            reactions_button_counter_2 += 1\n",
    "        else:\n",
    "            count += 1\n",
    "            print('error')\n",
    "            continue\n",
    "\n",
    "        scroll_reactions_window(driver)\n",
    "\n",
    "        time.sleep(random.uniform(2,6))\n",
    "        #кнопка с лайками нажата, парсим всю инфу со страницы в новый суп\n",
    "        reactions_window_soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        #все лайкнувшие юзеры будут тут, но нужно скролить до конца\n",
    "        reactions_window_soup = reactions_window_soup.find('ul','artdeco-list artdeco-list--offset-1')\n",
    "        usernames = []\n",
    "        user_links = []\n",
    "        job_titles = []\n",
    "        for i in reactions_window_soup.find_all('a', 'link-without-hover-state ember-view'):\n",
    "            try:\n",
    "                name = i.find('span',{'dir':'ltr'}).text\n",
    "            except:\n",
    "                name = 'Not defined'\n",
    "            usernames.append(name)\n",
    "            user_link = i['href']\n",
    "            user_links.append(user_link)\n",
    "            job_title = i.find('div','artdeco-entity-lockup__caption ember-view').text.strip()\n",
    "            job_titles.append(job_title)\n",
    "\n",
    "        # упаковываем результаты в словарь\n",
    "        for i,v in enumerate(user_links):\n",
    "            result_dict[count][1][v] = [usernames[i],job_titles[i]]\n",
    "        #закрываем страницу с лайками и добавляем к каунтеру 1\n",
    "        driver.find_element(By.CLASS_NAME, 'artdeco-button__icon').click()\n",
    "        count += 1\n",
    "\n",
    "        time.sleep(random.uniform(2,7))\n",
    "        #создаем 2 списка, в каждом из них элементы при нажатии на которые открываются лайки\n",
    "        likes_list_1 = driver.find_elements(By.CLASS_NAME, 'social-details-social-counts__reactions-count')\n",
    "        likes_list_2 = driver.find_elements(By.CLASS_NAME, 'social-details-social-counts__social-proof-container')\n",
    "        time.sleep(random.uniform(5,10))\n",
    "        soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        \n",
    "        return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee74163-609d-43f0-b28c-c1cc12416a72",
   "metadata": {},
   "source": [
    "# Сбор информации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7eb1b-6254-4fdd-9836-9f896ced182d",
   "metadata": {},
   "source": [
    "Профили и информация по ним на примере поискового запроса \"senior developer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008e675-8133-4622-a97e-1c41b8f366eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'senior developer'\n",
    "\n",
    "profiles, u_posts, u_reactions = parse_profiles(query,\n",
    "                                                'my_login.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d544338-25e1-4a51-aabd-613ecab4ed66",
   "metadata": {},
   "source": [
    "Посты и информация о реакциях на примере запроса \"менторство в it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950989e5-04d8-483f-a728-d85cec37bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'менторство в it'\n",
    "\n",
    "posts_results = parse_posts_by_query(query,\n",
    "                                     'my_login.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
